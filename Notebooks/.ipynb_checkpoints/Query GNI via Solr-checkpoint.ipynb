{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query GNI via Solr\n",
    "\n",
    "Jenna Jordan\n",
    "\n",
    "22 January 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions\n",
    "\n",
    "- `query_solr()` will take a plain text query and the desired fields and return the results of the query as a list of dicts\n",
    "- `to_daily_timeseries()` will then take this list of dicts (with the presumed fields of `aid`, `publication_date`, and `publisher`) and transform it into a timeseries, with article counts for each day. A column name for the query should be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fields = ['aid', 'publication_date', 'publisher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_solr(query:str, fields:list):\n",
    "        \n",
    "    # the url to send the query to\n",
    "    baseurl = \"http://kaboodle.clinecenter.illinois.edu:8983/solr/index/select?\"\n",
    "    \n",
    "    # incorporate parameters\n",
    "    q = \"q=\" + query + \"AND source_name:BulkLexisNexis\"\n",
    "    fl = \"fl=\" + \",\".join(fields)\n",
    "    \n",
    "    # define initial url for query\n",
    "    rows = 0\n",
    "    init_query = baseurl + fl + \"&\" + q + \"&rows=\" + str(rows)\n",
    "    \n",
    "    # figure out how many results to get\n",
    "    time.sleep(5)\n",
    "    init_query_results = requests.get(init_query)\n",
    "    \n",
    "    # exception handling in case something goes wrong with the query\n",
    "    if init_query_results.status_code == 200:\n",
    "        records_found = init_query_results.json()['response']['numFound']\n",
    "    else:\n",
    "        records=[]\n",
    "    \n",
    "    # now get the actual data\n",
    "    rows = records_found\n",
    "    final_query = baseurl + fl + \"&\" + q + \"&rows=\" + str(rows)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    final_query_results = requests.get(final_query)\n",
    "    \n",
    "    # exception handling in case something goes wrong with the query\n",
    "    if final_query_results.status_code == 200:\n",
    "        records = final_query_results.json()['response']['docs']\n",
    "    else:\n",
    "        records = []\n",
    "    \n",
    "    \n",
    "    if len(records) > 0:\n",
    "        print(\"Success! Your query returned \" + str(records_found) + \" documents.\")\n",
    "        return records\n",
    "    else:\n",
    "        print(\"Something went wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_daterange = pd.date_range(start='1977-01-01', end='2019-08-18')\n",
    "SWB_daterange = pd.date_range(start='1979-01-01', end='2019-08-18')\n",
    "AFP_daterange = pd.date_range(start='1991-05-05', end='2019-08-18')\n",
    "XGNS_daterange = pd.date_range(start='1977-01-01', end='2019-08-18')\n",
    "NYT_daterange = pd.date_range(start='1980-06-01', end='2019-08-18')\n",
    "WP_daterange = pd.date_range(start='1977-01-01', end='2019-08-18')\n",
    "UPI_daterange = pd.date_range(start='1980-09-26', end='2019-08-16')\n",
    "DPA_daterange = pd.date_range(start='1994-07-03', end='2019-08-18')\n",
    "IPS_daterange = pd.date_range(start='2010-01-13', end='2019-07-17')\n",
    "\n",
    "publishers = [{'name': 'BBC Monitoring: International Reports', 'abbr': 'SWB', 'dates': SWB_daterange}, \n",
    "              {'name': 'The New York Times', 'abbr': 'NYT', 'dates': NYT_daterange},\n",
    "              {'name': 'The Washington Post', 'abbr': 'WP', 'dates': WP_daterange},\n",
    "              {'name': 'The Associated Press', 'abbr': 'AP', 'dates': AP_daterange},\n",
    "              {'name': 'Agence France Presse - English', 'abbr': 'AFP', 'dates': AFP_daterange},\n",
    "              {'name': 'Xinhua General News Service', 'abbr': 'XGNS', 'dates': XGNS_daterange},\n",
    "              {'name': 'UPI (United Press International)', 'abbr': 'UPI', 'dates': UPI_daterange},\n",
    "              {'name': 'dpa international (Englischer Dienst)', 'abbr': 'DPA', 'dates': DPA_daterange},\n",
    "              {'name': 'Inter Press Service', 'abbr': 'IPS', 'dates': IPS_daterange}]\n",
    "\n",
    "all_dfs = []\n",
    "for pub in publishers:\n",
    "    df = pd.DataFrame(pub['dates'], columns=['publication_date'])\n",
    "    df['publisher'] = pub['name']\n",
    "    df['publisher'] = df['publisher'].astype('category')\n",
    "    all_dfs.append(df)\n",
    "base_ts = pd.concat(all_dfs)\n",
    "\n",
    "def to_daily_timeseries(results:list, query_name:str):\n",
    "    df = pd.DataFrame(results)\n",
    "    df['publication_date'] = pd.to_datetime(df['publication_date'].astype('str').str[:10], format='%Y-%m-%d')\n",
    "    df_gb = df.groupby(['publication_date', 'publisher']).agg({'aid':'nunique'}) \\\n",
    "            .reset_index().rename(columns={'aid':query_name})\n",
    "    final_df = base_ts.merge(df_gb, on=['publication_date', 'publisher'], how='left').fillna(0) \\\n",
    "               .sort_values(['publication_date', 'publisher']).set_index(['publication_date', 'publisher'])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_queries(queries:list):\n",
    "\n",
    "    base_df = base_ts.sort_values(['publication_date', 'publisher']).set_index(['publication_date', 'publisher'])\n",
    "    \n",
    "    # loop through the queries, merging each into the base and then updating the base\n",
    "    for q in queries:\n",
    "        result = query_solr(q['query'], ts_fields)\n",
    "        new_table = to_daily_timeseries(result, q['name'])\n",
    "        base_df = base_df.merge(new_table, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "- only the content query should be included - the corpus is already specified in the function\n",
    "- enclose the entire query field in triple quotes\n",
    "- enclose the entire content query in parentheses\n",
    "- no spaces in the query name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = [\n",
    "    {'name': 'BLN_total', \n",
    "     'query': \n",
    "    \"\"\"\n",
    "    (content:*)\n",
    "    \"\"\"},\n",
    "    {'name': 'climate_change', \n",
    "     'query': \n",
    "    \"\"\"\n",
    "    (content:\"climate change\")\n",
    "    \"\"\"},\n",
    "     {'name': 'global_warming', \n",
    "     'query': \n",
    "    \"\"\"\n",
    "    (content:\"global warming\")\n",
    "    \"\"\"},\n",
    "     {'name': 'pollinator_population', \n",
    "     'query': \n",
    "    \"\"\"\n",
    "    (content:(insect* OR pollinator* OR bee* OR honeybee* OR butterfl* OR moth*) AND (population OR *diversity OR biomass OR ecolog* OR ecosystem* OR entomolog*))\n",
    "    \"\"\"},\n",
    "    {'name': 'pollinator_crisis', \n",
    "     'query': \n",
    "    \"\"\"\n",
    "    (content:(insect* OR pollinator* OR bee* OR honeybee* OR butterfl* OR moth*) AND (population OR *diversity OR biomass OR ecolog* OR ecosystem* OR entomolog*) AND (crisis OR \"colony collapse\" OR apocalypse OR extinct* OR declin* OR drop OR decreas* OR disappear*))\n",
    "    \"\"\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your query returned 33381218 documents.\n",
      "Success! Your query returned 157645 documents.\n",
      "Success! Your query returned 76736 documents.\n",
      "Success! Your query returned 918284 documents.\n",
      "Success! Your query returned 304088 documents.\n"
     ]
    }
   ],
   "source": [
    "df = run_all_queries(all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmap = {}\n",
    "for pub in publishers:\n",
    "    pubmap[pub['name']]= pub['abbr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.publisher = df.publisher.map(pubmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>BLN_total</th>\n",
       "      <th>climate_change</th>\n",
       "      <th>global_warming</th>\n",
       "      <th>pollinator_population</th>\n",
       "      <th>pollinator_crisis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>AP</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>WP</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>XGNS</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1977-01-02</td>\n",
       "      <td>AP</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977-01-02</td>\n",
       "      <td>WP</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113056</th>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>AP</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113057</th>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>NYT</td>\n",
       "      <td>372.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113058</th>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>WP</td>\n",
       "      <td>349.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113059</th>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>XGNS</td>\n",
       "      <td>223.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113060</th>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>DPA</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113061 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication_date publisher  BLN_total  climate_change  global_warming  \\\n",
       "0            1977-01-01        AP        9.0             0.0             0.0   \n",
       "1            1977-01-01        WP      112.0             0.0             0.0   \n",
       "2            1977-01-01      XGNS       39.0             0.0             0.0   \n",
       "3            1977-01-02        AP       37.0             0.0             0.0   \n",
       "4            1977-01-02        WP      195.0             0.0             0.0   \n",
       "...                 ...       ...        ...             ...             ...   \n",
       "113056       2019-08-18        AP      202.0             1.0             1.0   \n",
       "113057       2019-08-18       NYT      372.0             9.0             1.0   \n",
       "113058       2019-08-18        WP      349.0            19.0             4.0   \n",
       "113059       2019-08-18      XGNS      223.0             3.0             0.0   \n",
       "113060       2019-08-18       DPA      161.0             0.0             1.0   \n",
       "\n",
       "        pollinator_population  pollinator_crisis  \n",
       "0                         3.0                0.0  \n",
       "1                         6.0                4.0  \n",
       "2                         1.0                1.0  \n",
       "3                         2.0                1.0  \n",
       "4                         4.0                0.0  \n",
       "...                       ...                ...  \n",
       "113056                    8.0                2.0  \n",
       "113057                   47.0               23.0  \n",
       "113058                   30.0               14.0  \n",
       "113059                    7.0                2.0  \n",
       "113060                    1.0                0.0  \n",
       "\n",
       "[113061 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/queries_4Feb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beepocalypse-project",
   "language": "python",
   "name": "beepocalypse-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
